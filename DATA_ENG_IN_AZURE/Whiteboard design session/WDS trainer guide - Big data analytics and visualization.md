![Microsoft Cloud Workshop](https://github.com/Microsoft/MCW-Template-Cloud-Workshop/raw/main/Media/ms-cloud-workshop.png 'Microsoft Cloud Workshop')

<div class="MCWHeader1">
Big data analytics and visualization
</div>

<div class="MCWHeader2">
Whiteboard design session trainer guide
</div>

<div class="MCWHeader3">
November 2021
</div>

Information in this document, including URL and other Internet Web site references, is subject to change without notice. Unless otherwise noted, the example companies, organizations, products, domain names, e-mail addresses, logos, people, places, and events depicted herein are fictitious, and no association with any real company, organization, product, domain name, e-mail address, logo, person, place or event is intended or should be inferred. Complying with all applicable copyright laws is the responsibility of the user. Without limiting the rights under copyright, no part of this document may be reproduced, stored in or introduced into a retrieval system, or transmitted in any form or by any means (electronic, mechanical, photocopying, recording, or otherwise), or for any purpose, without the express written permission of Microsoft Corporation.

Microsoft may have patents, patent applications, trademarks, copyrights, or other intellectual property rights covering subject matter in this document. Except as expressly provided in any written license agreement from Microsoft, the furnishing of this document does not give you any license to these patents, trademarks, copyrights, or other intellectual property.

The names of manufacturers, products, or URLs are provided for informational purposes only and Microsoft makes no representations and warranties, either expressed, implied, or statutory, regarding these manufacturers or the use of the products with any Microsoft technologies. The inclusion of a manufacturer or product does not imply endorsement of Microsoft of the manufacturer or product. Links may be provided to third party sites. Such sites are not under the control of Microsoft and Microsoft is not responsible for the contents of any linked site or any link contained in a linked site, or any changes or updates to such sites. Microsoft is not responsible for webcasting or any other form of transmission received from any linked site. Microsoft is providing these links to you only as a convenience, and the inclusion of any link does not imply endorsement of Microsoft of the site or the products contained therein.

Â© 2021 Microsoft Corporation. All rights reserved.

Microsoft and the trademarks listed at <https://www.microsoft.com/en-us/legal/intellectualproperty/Trademarks/Usage/General.aspx> are trademarks of the Microsoft group of companies. All other trademarks are property of their respective owners.

# Big data analytics and visualization whiteboard design session trainer guide

**Contents**

<!-- TOC -->

- [Big data analytics and visualization whiteboard design session trainer guide](#big-data-analytics-and-visualization-whiteboard-design-session-trainer-guide)
- [Trainer information](#trainer-information)
  - [Role of the trainer](#role-of-the-trainer)
  - [Whiteboard design session flow](#whiteboard-design-session-flow)
  - [Before the whiteboard design session: How to prepare](#before-the-whiteboard-design-session-how-to-prepare)
  - [During the whiteboard design session: Tips for an effective whiteboard design session](#during-the-whiteboard-design-session-tips-for-an-effective-whiteboard-design-session)
- [Big data analytics and visualization whiteboard design session student guide](#big-data-analytics-and-visualization-whiteboard-design-session-student-guide)
  - [Abstract and learning objectives](#abstract-and-learning-objectives)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study)
    - [Customer situation](#customer-situation)
    - [Customer needs](#customer-needs)
    - [Customer objections](#customer-objections)
    - [Infographic for common scenarios](#infographic-for-common-scenarios)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution)
  - [Step 3: Present the solution](#step-3-present-the-solution)
  - [Wrap-up](#wrap-up)
  - [Additional references](#additional-references)
- [Big data analytics and visualization whiteboard design session trainer guide](#big-data-analytics-and-visualization-whiteboard-design-session-trainer-guide-1)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study-1)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution-1)
  - [Step 3: Present the solution](#step-3-present-the-solution-1)
  - [Wrap-up](#wrap-up-1)
  - [Preferred target audience](#preferred-target-audience)
  - [Preferred solution](#preferred-solution)
  - [Checklist of preferred objection handling](#checklist-of-preferred-objection-handling)
  - [Customer quote (to be read back to the attendees at the end)](#customer-quote-to-be-read-back-to-the-attendees-at-the-end)

<!-- /TOC -->

# Trainer information

Thank you for taking the time to support the whiteboard design sessions as a trainer!

## Role of the trainer

An amazing trainer:

- Creates a safe environment in which learning can take place.

- Stimulates the participant's thinking.

- Involves the participant in the learning process.

- Manages the learning process (on time, on topic, and adjusting to benefit participants).

- Ensures individual participant accountability.

- Ties it all together for the participant.

- Provides insight and experience to the learning process.

- Effectively leads the whiteboard design session discussion.

- Monitors quality and appropriateness of participant deliverables.

- Effectively leads the feedback process.

## Whiteboard design session flow

Each whiteboard design session uses the following flow:

**Step 1: Review the customer case study (15 minutes)**

**Outcome**

Analyze your customer's needs.

- Customer's background, situation, needs, and technical requirements.

- Current customer infrastructure and architecture.

- Potential issues, objectives, and blockers.

**Step 2: Design a proof of concept solution (60 minutes)**

Outcome: Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

- Determine your target customer audience.

- Determine customer's business needs to address your solution.

- Design and diagram your solution.

- Prepare to present your solution.

**Step 3: Present the solution (30 minutes)**

Outcome: Present solution to your customer.

- Present solution.

- Respond to customer objections.

- Receive feedback.

**Wrap-up (15 minutes)**

- Review preferred solution.

## Before the whiteboard design session: How to prepare

Before conducting your first whiteboard design session:

- Read the Student guide (including the case study) and Trainer guide.

- Become familiar with all key points and activities.

- Plan the point you want to stress, which questions you want to drive, transitions, and be ready to answer questions.

- Before the whiteboard design session, discuss the case study to pick up more ideas.

- Make notes for later.

## During the whiteboard design session: Tips for an effective whiteboard design session

**Refer to the Trainer guide** to stay on track and observe the timings.

**Do not expect to memorize every detail** of the whiteboard design session.

When participants are doing activities, you can **look ahead to refresh your memory**.

- **Adjust activity and whiteboard design session pace** as needed to allow time for presenting, feedback, and sharing.

- **Add examples, points, and stories** from your own experience. Think about stories you can share that help you make your points clearly and effectively.

- **Consider creating a "parking lot"** to record issues or questions raised that are outside the scope of the whiteboard design session or can be answered later. Decide how you will address these issues, so you can acknowledge them without being derailed by them.

**Have fun**! Encourage participants to have fun and share!

**Involve your participants.** Talk and share your knowledge but always involve your participants, even while you are the one speaking.

**Ask questions** and get them to share to fully involve your group in the learning process.

**Ask first**, whenever possible. Before launching into a topic, learn your audience's opinions about it and experiences with it. Asking first enables you to assess their level of knowledge and experience and leaves them more open to what you are presenting.

**Wait for responses**. If you ask a question such as, "What's your experience with (fill in the blank)?" then wait. Do not be afraid of a bit of silence. If you leap into the silence, your participants will feel you are not serious about involving them and will become passive. Give participants a chance to think, and if no one answers, patiently ask again. You will usually get a response.

# Big data analytics and visualization whiteboard design session student guide

## Abstract and learning objectives

In this whiteboard design session, you will work with a group to design a solution for ingesting and preparing historic flight delay and weather data and creating, training, and deploying a machine learning model that can predict flight delays.

At the end of this whiteboard design session, you will have learned how to include a web application that obtains weather forecasts from a 3rd party, collects flight information from end-users, and sends that information to the deployed machine learning model for scoring. Part of the exercise will include providing visualizations of historic flight delays and orchestrating the collection and batch scoring of historic and new flight delay data.

## Step 1: Review the customer case study 

**Outcome**

Analyze your customer's needs.

Timeframe: 15 minutes

Directions: With all participants in the session, the facilitator/SME presents an overview of the customer case study along with technical tips.

1. Meet your team members and trainer.

2. Read all directions for steps 1-3 in the student guide.

3. As a team, review the following customer case study.

### Customer situation

Margie's Travel (MT) provides concierge services for business travelers. In an increasingly crowded market, they are always looking for ways to differentiate themselves and add value to their corporate customers.

MT is investigating ways to capitalize on their existing data assets to provide new insights that provide them a strategic advantage against their competition. In planning their product, they heard much fanfare about machine learning. They came up with the idea of using predictive analytics to help customers best select their travels based on the likelihood of a delay. When reviewing their customer transaction histories, they discovered that their most premium customers often book their travel within seven days of departure. In speaking with customer service, they learned that these customers often ask questions like, "I don't have to be there until Tuesday, so is it better for me to fly out on Sunday or Monday?"

While there are many factors that customer service uses to tailor their guidance to the customer (such as cost and travel duration), MT believes an innovative solution might come in the form of giving the customer an assessment of the risk of encountering flight delays. The customer may choose to book with a narrower travel window for low-risk flights, giving them more precious time at home and less on the road spent arriving too early to a destination. MT is interested in applying data science to the problem to discover if the weather forecast and their historical flight delay data could provide meaningful input into the customer's decision-making process.

MT plans to pilot this solution internally, whereby the small population of customer support who service MT's premium tier of business travelers would begin using the solution and offering it as an additional data point for travel optimization. They would like to provide their customer support agents a web-based solution that enables them to map the predicted delays for a particular customer's departure airport(s) of choice.

MT has over 30 years of historical flight data provided to them by the United States Department of Transportation (USDOT), which among other data points, includes flight delay information for every flight. The data arrives in flat, comma-separated value (CSV) files with a schema of the following:

(Year, Month, DayOfMonth, Airline, TailNum, FlightNum, OriginAirport, DestinationAirport, ScheduledDepartureTime, ActualDepartureTime, ScheduledArrivalTime, DepartureDelay, AirTime, Distance, Cancelled, CancellationCode)

In addition, for all data since 2003, each row includes new fields describing the type of delay experienced, where the value for each type is the number of minutes the delay was experienced for that source of delay:

(CarrierDelay, WeatherDelay, NationalAirSystemDelay, SecurityDelay, LateAircraftDelay)

They receive updates to this data monthly, where the flight data and other related files total about 1 GB. In total, their solution currently manages about 2 TB worth of data.

Additionally, they receive current and forecasted weather data from a third-party service. This service gives them the ability to receive weather forecasts around any airport and provides forecasts for up to 10 days. They have a history of each flight's historical weather condition as CSV files but acquiring the weather forecasts requires a call to a REST API that returns a JSON (JavaScript Object Notation) structure. Each airport of interest needs to be queried individually. An excerpt of the weather forecast for a single day at the Seattle-Tacoma International airport is as follows:

```json
{
  "date": {
    "epoch": "1444701600",
    "pretty": "7:00 PM PDT on October 12, 2015",
    "day": 12,
    "month": 10,
    "year": 2015,
    "yday": 284,
    "hour": 19,
    "min": "00",
    "sec": 0,
    "ampm": "PM",
    "tz_short": "PDT",
    "tz_long": "America/Los_Angeles"
  },
  "high": {
    "fahrenheit": "64",
    "celsius": "18"
  },
  "low": {
    "fahrenheit": "54",
    "celsius": "12"
  },
  "conditions": "Overcast",
  "maxwind": {
    "mph": 15,
    "kph": 24,
    "dir": "SSW",
    "degrees": 209
  },
  "avewind": {
    "mph": 10,
    "kph": 16,
    "dir": "SSW",
    "degrees": 209
  },
  "avehumidity": 70,
  "maxhumidity": 0,
  "minhumidity": 0
}
```

Jack Tradewinds, the CIO of MT, is looking to modernize their data story. He has heard a great deal of positive news about Spark SQL on Azure Databricks and its ability to query exactly the type of files he has in a performant way, but also in a way that is more familiar to his analysts and developers because they are all familiar with the SQL syntax that it supports. He would love to understand if they can move this data away from their on-premises data center into the cloud and enhance their ability to load, process, and analyze it going forward. Given his long-standing relationship with Microsoft, he would like to see if Azure can meet his needs.

### Customer needs

1. Want to modernize their analytics platform without sacrificing the ability to query their data using SQL.

2. Need an approach that can store all their data, including the unmodified source data and the cleansed data they query for production purposes.

3. Want to understand how they will load their large quantity of historical data into Azure.

4. Need to query the weather forecast and use it as input to their flight delay predictions.

5. Desire a proof of concept (PoC) machine learning model that takes as input their historical data on flight delays and weather conditions to identify whether a flight is likely to be delayed or not.

6. Need web-based visualizations of the flight delay predictions.

### Customer objections

1. We have heard that creating a machine learning model takes a month to build and another 2-3 months to operationalize to be useable from our production systems. Is this true?

2. Once our model is operationalized, how do we retrain and redeploy it? Will this process break clients currently accessing the deployed model?

3. Can we query flat files in the file system using SQL?

4. Does Azure provide anything that would speed up querying (and exploration) files in Hadoop Distributed File Systems (HDFS)?

5. Does Azure provide any tools for visualizing our data? Ideally, access to these could be managed with Active Directory.

6. Can we use Azure Active Directory accounts for our users? If so, can we restrict who can access Azure Databricks when they can access it, require two-factor authentication, and restrict access if there is suspicious activity on their account?

7. Is Azure Databricks our only option for running SQL on Hadoop solutions in Azure?

8. We have heard of Azure Data Lake, but we are not clear about whether this is currently a good fit for our PoC solution or whether we should be using it for interactive analysis of our data.

9. We are hiring a data scientist who prefers to use MLflow to track model training run metrics and artifacts. Can the proposed Azure-based solution support this library?

### Infographic for common scenarios

![The Data Analytics diagram is broken into three sections: On-Premises, Azure, and End Users. On-Premises includes a Web Server with log files and an end-user with a computer and a portable device. The Azure section contains three parts: Generation (Azure website and log files), Storage (Azure SQL Database and Blob Storage), and Data Processing (SQL Data Warehouse, Machine Learning, and HDInsight (Hadoop). The End Users section has Business Intelligence and End Users with portable devices.](media/common-scenarios.png 'Data Analytics diagram')

## Step 2: Design a proof of concept solution

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 60 minutes

**Business needs**

Directions: With your team, answer the following questions and be prepared to present your solution to others:

1. Who will you present this solution to? Who is your target customer audience? Who are the decision makers?

2. What customer business needs do you need to address with your solution?

**Design**

Directions: With your team, respond to the following questions:

_High-level architecture_

1. Without getting into the details (the following sections will address the details), diagram your initial vision for handling the top-level requirements for data loading, data preparation, storage, machine learning modeling, and reporting. You will refine this diagram as you proceed.

_Data loading_

1. How would you recommend that MT get their historical data into Azure? What services would you suggest, and what are the specific steps they would need to take to prepare the data, to transfer the data, and where would the loaded data land?

2. Update your diagram with the data loading process with the steps you identified.

_Data preparation_

1. What service would you recommend MT capitalize on to explore the flat files they get from the USDOT using SQL?

2. What specific configuration would you use? What components of Azure Databricks would you use to allow MT analysts to query and prep the data? How would they author and execute these data prep tasks?

3. How would you suggest MT integrate weather forecast data?

_Machine learning modeling_

1. What technology would you recommend that MT use for implementing their machine learning model?

2. How would you guide MT to load data so that the machine learning model can process it?

3. What category of machine learning algorithm would you recommend to MT to construct their model? For this scenario, your option is clustering, regression, or two-class classification. Why?

4. Assuming you selected an algorithm that requires training, address the following model design questions:

   a. What is the high-level flow of your machine learning model?

   b. What attributes of the flight and weather data do you think MT should use in predicting flight delays? How would you recommend that MT identify the columns that provide the most predictive value in determining if a flight will be delayed? Be specific on the particular modules or libraries they could use and how they would apply them against the data.

   c. Some of the data may need a little touching up: columns need to be removed; data types need to be changed. How would these steps be applied in your model?

   d. How would you recommend MT measure the success of their model?

_Operationalizing machine learning_

1. How can MT release their model for production use and avoid their concerns about extremely long delays operationalizing the model? Be specific on how your model is packaged, hosted, and invoked.

2. MT has shown interest in not only scoring a flight at a time (based on a customer's request), but also doing scoring in large chunks so that they could show summaries of predicted flight delays across the United States. What changes would you need to make to your ML model to support this?

3. MT wants a cost-effective data store to serve the results from the batch scoring process. The reporting and visualization service should use this data store as opposed to connecting to costly compute clusters. Which data store do you propose, and how will the compute environment that performs batch scoring securely connect to this serving layer without exposing connection strings or other secrets?

_Visualization and reporting_

1. Is Power BI an option for MT to use in visualizing the flight delays?

2. If so, explain:

   a. How would MT load the data and plot it on a map? What specific components would you use, and how would you configure them to display the data?

   b. If they need to make minor changes, such as a change to the data types of a column in the model, how would they perform this in Power BI?

   c. How could they secure access to these reports to only their internal customer service agents?

3. MT wants a way to monitor their data pipeline, including ETL, data preparation, and model training activities. How can they capture and visualize metrics to watch for problems such as performance bottlenecks? How can they quickly access the logs from a single location?

**Prepare**

Directions: As a team:

1. Identify any customer needs that are not addressed with the proposed solution.

2. Identify the benefits of your solution.

3. Determine how you will respond to the customer's objections.

Prepare a 15-minute chalk-talk style presentation to the customer.

## Step 3: Present the solution

**Outcome**

Present a solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 30 minutes

**Presentation**

Directions:

1. Pair with another team.

2. One group is the Microsoft team and the other is the customer.

3. The Microsoft team presents their proposed solution to the customer.

4. The customer makes one of the objections from the list of objections.

5. The Microsoft team responds to the objection.

6. The customer team gives feedback to the Microsoft team.

7. Switch roles and repeat Steps 2-6.

## Wrap-up

Timeframe: 15 minutes

Directions: Reconvene with the larger group to hear the facilitator/SME share the preferred solution for the case study.

## Additional references

|                                 |                                                                                                    |
| ------------------------------- | :------------------------------------------------------------------------------------------------: |
| **Description**                 |                                             **Links**                                              |
| Azure solution architectures    |                    <https://azure.microsoft.com/en-us/solutions/architecture/>                     |
| Azure Machine Learning services |                 <https://docs.microsoft.com/en-us/azure/machine-learning/service/>                 |
| Machine Learning algorithms     | <https://azure.microsoft.com/en-us/documentation/articles/machine-learning-algorithm-cheat-sheet/> |
| Azure Data Factory              |                   <https://docs.microsoft.com/azure/data-factory/introduction/>                    |
| Azure Databricks                |                    <https://docs.microsoft.com/en-us/azure/azure-databricks//>                     |
| Power BI                        |       <https://support.powerbi.com/knowledgebase/articles/430814-get-started-with-power-bi/>       |
| Travel data                     |                           <https://www.transtats.bts.gov/homepage.asp/>                            |
| Weather data                    |                                  <https://openweathermap.org/api/one-call-api>                                   |
| ARM Templates                   |   <https://docs.microsoft.com/azure/azure-resource-manager/resource-group-authoring-templates/>    |
| Azure AD Conditional Access     |              <https://docs.microsoft.com/azure/active-directory/conditional-access/>               |
| Azure SQL Database | <https://docs.microsoft.com/azure/azure-sql/database/sql-database-paas-overview> |
| Write to Azure SQL Database from a DataFrame | <https://docs.microsoft.com/azure/databricks/data/data-sources/sql-databases#write-data-to-jdbc> |
| Azure Key Vault | <https://docs.microsoft.com/azure/key-vault/key-vault-overview> |
| Azure Monitor | <https://docs.microsoft.com/azure/azure-monitor/overview> |

# Big data analytics and visualization whiteboard design session trainer guide

## Step 1: Review the customer case study

- Check in with your participants to introduce yourself as the trainer.

- Ask, "What questions do you have about the customer case study?"

- Briefly review the steps and timeframes of the whiteboard design session.

- Ready, set, go! Let participants begin.

## Step 2: Design a proof of concept solution

- Check in with your teams to ensure that they are transitioning from step to step on time.

- Provide feedback on their responses to the business needs and design.

  - Try asking questions first that will lead the participants to discover the answers on their own.

- Provide feedback for their responses to the customer's objections.

  - Try asking questions first that will lead the participants to discover the answers on their own.

## Step 3: Present the solution

- Determine which groups will be paired together before Step 3 begins.

- For the first round, assign one group as the presenting team and the other as the customer.

- Have the presenting team present their solution to the customer team.

  - Have the customer team provide one objection for the presenting team to respond to.

  - The presentation, objections, and feedback should take no longer than 15 minutes.

  - If needed, the trainer may also provide feedback.

## Wrap-up

- Have participants reconvene with the larger session group to hear the facilitator/SME share the following preferred solution.

## Preferred target audience

Jack Tradewinds, CIO of Margie's Travel (MT)

The primary audience is the business decision-makers and technology decision-makers. From the case study scenario, this would include the Director of Analytics. Usually, we talk to the infrastructure managers who report to the chief information officers (CIOs), or application sponsors (like a vice president \[VP\] line of business \[LOB\], or chief marketing officer \[CMO\]), or to those that represent the business unit IT or developers that report to application sponsors.

## Preferred solution

_High-level architecture_

1. Without getting into the details (the following sections will address the details), diagram your initial vision for handling the top-level requirements for data loading, data preparation, storage, machine learning modeling, and reporting. You will refine this diagram as you proceed.

   After speaking with its supportive team at Microsoft, MT decided that Azure would be the right choice for their platform. They decided to load data into blob storage; explore and prepare it using Spark SQL on Azure Databricks; train and serve a model with Azure Databricks; and visualize the result using a map visualization in Power BI. Batch-scored predictions are stored in Azure SQL Database, which operates as the serving layer for Power BI reports. Azure Key Vault is used as a secret store to centrally manage and securely provide access to secrets, such as connection strings and application keys, to Azure Databricks notebooks and other services, such as Azure Data Factory and the Web App. Azure Monitor provides centralized monitoring and logging of all Azure components of the solution.

   ![The high-level overview diagram of the end-to-end solution is displayed. Flight delay data and historical airport weather data are provided to Azure Data Factory. Azure Data Factory provides this data to both blob storage and Azure Databricks. Azure Databricks scores the data and saves the results to an Azure SQL Database. Azure Databricks also creates, trains, and serves a machine learning model that is consumed by the web portal. The web portal also consumes 3rd party API data for forecasted weather. Map data visualization is provided by Power BI using web portal information and the Azure SQL database.](media/high-level-overview.png 'High-level overview diagram')

   This solution is only one of many possible, viable approaches.  In a production scenario, it may make sense to use Azure Data Lake Storage Gen2 instead of Blob Storage, as it provides additional flexibility and security options over Azure Blob Storage.  Another potential swap would be to incorporate Azure Synapse Analytics, using Synapse Pipelines instead of Azure Data Factory to move data from on-premises storage into ADLS Gen2.  Databricks can then read data from ADLS Gen2, perform machine learning operations, and write data back to ADLS Gen2.  Finally, instead of using Azure SQL Database or directly querying Spark tables in Databricks, another viable option would be to use the Azure Synapse Analytics serverless SQL pool to expose data to Power BI.

_Data loading_

1. How would you recommend that MT get their historical data into Azure? What services would you suggest, and what are the specific steps they would need to take to prepare the data, to transfer the data, and where would the loaded data land?

   MT should consider using Azure Data Factory (ADF) for copying their historical data into Azure. By setting up a continuous pipeline containing a copy activity configured to copy time partitioned source data, they could pull all their historical information, as well as ingest any future data, into Azure blob storage through a scheduled and continuously running pipeline. Because their historical data is stored on-premises, MT would need to install and configure an Azure Data Factory Integration Runtime (formerly known as a Data Management Gateway). Once in place, this would allow ADF to copy data from their local data store to a container in blob storage. Their pipeline would be configured to run monthly, as that is the frequency at which new data is received, and this would still allow for all their historical data to be copied without delay.

2. Update your diagram with the data loading process with the steps you identified.

   ![The Data loading process diagram begins with Flight Delay Data and Historical Airport Weather Data flat files. An arrow points from there to ADF Copy Pipeline, which in turn points to Blob Storage.](media/data-loading.png 'Data loading process')

_Data preparation_

1. What service would you recommend MT capitalize on to explore the flat files they get from the United States Department of Transportation (USDOT) using SQL?

   Because of their interest in exploring and preparing the flat files using SQL, the best choice is to use Azure Databricks. It is Microsoft's premier Apache Spark hosted solution that will also be the platform for building and training the machine learning model for flight delay predictions, based on the same data that it was used to prepare. This offers a single platform for data preparation and machine learning within a collaborative environment that appeals to data scientists and data engineers.

2. What specific configuration would you use? What components of Azure Databricks would you use to allow MT analysts to query and prep the data? How would they author and execute these data prep tasks?

   They should use Spark SQL within Databricks notebooks. This gives them a simple, interactive interface that allows them to use programming languages such as Python, Scala, and R to prepare their data and train their data models. When a notebook is created, or an existing one is opened, they would need to attach it to a cluster. This provides them with a kernel that gives them a preset sqlContext that can be used to run Hive queries using Spark SQL syntax on the data, which allows them to leverage their existing SQL skills. They would need to create the appropriate external tables atop the flight delay files available in HDFS, which reads from the Azure Storage account attached to their Azure Databricks instance.

3. How would you suggest MT integrate weather forecast data?

   They could retrieve the weather forecast data from a third-party service that provides a REST API. An example of such a service is [OpenWeather](https://openweathermap.org/).

   ![The Historical Data Preparation flowchart begins with Flight Delay Data flat files and Historical Airport Weather Data flat files. Arrows point from the flat files to Blob Storage.](media/historical-data-preparation.png 'Historical Data Preparation flowchart')

_Machine learning modeling_

1. What technology would you recommend that MT use for implementing their machine learning model?

   The model will first be built and trained within an Azure Databricks notebook. The data scientists can use the programming language of their choice (Python, Scala, R, etc.) as well as Spark SQL to feature and fit the data into the chosen machine learning algorithm. Machine learning libraries such as Spark MLlib or SciKit-Learn can be used within the notebook to simplify things. Once the model is trained and tested with a sufficient amount of historical data, the model can be deployed to a web service. The model can also continue to be used within Azure Databricks for batch scoring.

2. How would you guide MT to load data, so the machine learning model can process it?

   The data used for training could be uploaded directly to Azure Databricks, creating a new table with the data stored in the Databricks File System (DBFS). Then the data can be accessed from this persistent global table using Spark SQL syntax or DataFrames. Alternately, if they need to train the model with a considerable amount of data, the data can be stored in Azure Storage. The Storage account can be mounted to an Azure Databricks cluster. From that point, the data can be accessed using the wasb/wasbs path and loaded into a DataFrame. Alternately, the data can be persisted into a global table so that it can be easily accessed using Spark SQL syntax and across cluster sessions.

3. What category of machine learning algorithm would you recommend to MT to construct their model? For this scenario, your options are clustering, regression, or two-class classification. Why?

   Given that MT only wants a binary prediction of flight delayed or flight not delayed, they should proceed with a two-class classification algorithm, such as logistic regression.

4. Assuming you selected an algorithm that requires training, address the following model design questions:

   a. What is the high-level flow of your machine learning model?

   The data contains several useful features (columns) that can be used for scoring. The numeric features may need to be normalized if there are values that are much higher than the others, like sea level pressure (averages around 30) compared to hourly precipitation (usually less than 1, mostly 0) and wind speed (rarely exceeds 10). This can give undue preference to the numeric features containing higher values overall. Next, the categorical columns (text-based), such as origin and destination airports, need to be encoded into numeric indices. They are essential features for predicting flight delays, but regression models do not know how to deal with text. You can use a process such as one-hot encoding.

   Once the features have been prepared, you will need to split the data so that the bulk of it (such as 70%) is used to train the model, and the rest of it is used to test or validate the model.

   b. What attributes of the flight and weather data do you think MT should use in predicting flight delays? How would you recommend that MT identify the columns that provide the most predictive value in determining if a flight will be delayed? Be specific on the modules or libraries they could use and how they would apply them against the data.

   There are multiple approaches MT could use to perform feature selection and to identify the data attributes that are the most helpful in accurately predicting a delay. They should start with any domain knowledge they have---this would likely point in the direction of the flight attribute's airline, departure airport, destination airport, and time of day and the weather attribute's wind speed, temperature, and precipitation conditions. Additionally, they should identify and remove fields that do not add value (i.e., because they are mostly empty or only have a constant value). They should use Python or R functions, such as `replace` or `na.omit`, respectively, to remove empty rows or replace constant values (such as 'M' for missing data) within the important feature columns. From there, they could construct a preliminary model and validate how it performs against the training data. In additional passes, they might choose to score their trained model's effectiveness against training and testing data sets to see whether their baseline score is improving or regressing.

   c. Some of the data may need a little touching up: columns need to be removed, data types need to be changed, etc. How would these steps be applied in your model?

   Data munging can be best accomplished using R or Python, languages familiar to data scientists and developers. These languages provide powerful data transformation capabilities and allow for flexibility in how data cleanup occurs while reducing ML models' overall complexity. These steps should be performed before featurization. You can use a Spark Pipeline to organize your data transformation steps in order.

   d. How would you recommend MT measure the success of their model?

   For a classification model, they should measure their model's success against a data set whereby a large portion of the data is used for training purposes. Still, a smaller set is reserved as examples that will be used to "test" the model to see how it performs against a known outcome. The output of the prediction as well as the "correct" output is passed to the one or more binary evaluation metrics which provide scores such as those from the Confusion Matrix, part of the MLlib MulticlassMetrics library, that give a perspective on accuracy and in what situations the models make mistakes.

   ![The ML Model Creation flowchart begins with prepared data stored in Blob storage. An Azure Databricks notebook is used to create and train the machine learning model from historical data. The trained model is exported and stored within the Azure Machine Learning service. Finally, the model is deployed on a Databricks cluster for model serving.](media/ml-model-creation.png 'ML Model Creation flowchart')

_Operationalizing machine learning_

1. How can MT release their model for production use and avoid their concerns about extremely long delays operationalizing the model? Be specific on how your model is packaged, hosted, and invoked.

   Once they have trained their model, it can be exported to any number of common file formats from within Azure Databricks. Because the model is already saved to DBFS and available within a notebook, the easiest way to deploy the model is to use the MLflow Model Serving capability in Azure Databricks to deploy a registered model on a hosted Databricks cluster. This creates a web service that any REST client can invoke, and the cluster can scale to meet demand as needed. This deployed web service takes the weather conditions and flight information as input and returns a response with the classification.

   ![In the On-Demand Delay Predictions flowchart, a Flight Delays Web Portal icon has one arrow labeled "1. Query for weather forecast," pointing to a 3rd Party API, labeled "Forecasted Airport Weather Data (API). A second arrow labeled "2. Query for delay prediction providing weather forecast and flight data" points to containerized AI Services.](media/on-demand-delay-predictions.png 'On-Demand Delay Predictions flowchart')

2. MT has shown interest in not only scoring a flight at a time (based on a customer's request), but also doing scoring in large chunks so that they could show summaries of predicted flight delays across the United States. What changes would you need to make to your ML model to support this?

   The model should be created so that data is prepared and featured consistently each time it is executed. This way, it can be used equally well for both interactive and batch scoring. The model should be provided within an Azure Databricks notebook that accepts input parameters that point to the data's file location that needs to be batch processed. This can be implemented via Azure Data Factory (v2) by adding a Linked Service to Azure Databricks and configuring the path and parameters to send to the notebook. Using ADF, they can apply the operational model to data as it is moved to the proper location in Azure storage. The scored results will be available in Blob storage after processing, and with the scheduled pipeline, new data will be processed automatically on the schedule indicated by the pipeline.

   The notebook that is executed also writes the summarized flight delay predictions to an Azure SQL Database. Power BI will use this data to display the predictions in a series of reports and dashboards.

3. MT wants a cost-effective data store to serve the results from the batch scoring process. The reporting and visualization service should use this data store as opposed to connecting to costly compute clusters. Which data store do you propose, and how will the compute environment that performs batch scoring securely connect to this serving layer without exposing connection strings or other secrets?

   The data can be stored in Azure SQL Database, which is a platform-as-a-service (PaaS) offering that minimizes operational overhead and provides a cost-effective way to store and serve the data for reporting and visualization. MT can create a JDBC connection to the Azure SQL Database within an Azure Databricks notebook and [write data from a DataFrame](https://docs.microsoft.com/azure/databricks/data/data-sources/sql-databases#write-data-to-jdbc), using the JDBC connection. They would use the `SaveMode.Append` write mode to append data to an existing SQL table rather than overwrite the table each time.

   To securely store the JDBC connection with SQL account details and other secrets, use Azure Key Vault, in addition to Key Vault-backed secret scopes within Azure Databricks. Azure Key Vault provides a service that allows you to centralize application secrets securely. The benefit of it being a centralized store of secrets is that you only need to define those secrets, like connection strings or account keys, in one place that several Azure services and custom applications can access.

   Azure Databricks has two types of secret scopes: Key Vault-backed and Databricks-backed. These secret scopes allow you to store secrets, such as database connection strings, securely. If someone tries to output a secret to a notebook, it is replaced by `[REDACTED]`. This helps prevent someone from viewing the secret or accidentally leaking it when displaying or sharing the notebook.

   ![An Azure Databricks notebook uses a JDBC connection that is securely stored in Azure Key Vault to connect to an Azure SQL Database and write bulk prediction data for reporting and visualization.](media/databricks-to-sql-database.png "Databricks to Sql Database")

_Visualization and reporting_

1. Is Power BI an option for MT to use in visualizing the flight delays?

   Yes, Power BI is a good option for MT. Power BI can perform what is called a Direct Query against Apache Spark hive data sources, as well as an Import query that copies the data into Power BI, managed datasets from Spark. However, connecting Power BI to an Azure Databricks cluster requires it to be running any time a report is displayed. A more cost-effective solution is to write the summarized data to an Azure SQL Database each time the notebook is executed during the batch scoring process. In this case, Power BI uses Direct Query against the Azure SQL Database instead, allowing MT to terminate clusters that are not in use.

2. If so, explain:

   a. How would MT load the data and plot it on a map? What specific components would you use, and how would you configure them to display the data?

   The data would be set up as a query against Azure SQL Database that loads the predicted flight delay for each airport location. They would use the map visualization to display a delay indicator for a particular flight at a particular airport.

   ![A map displays with two large dots over Los Angeles and San Diego.](media/visualization-map.png 'visualization map')

   b. If they need to make minor changes, such as a change to the data types of a column in the model, how would they perform this in Power BI?

   They could do this using the Query Editor component of the Power BI Desktop application. They could then upload this file to the Power BI service.

   c. How could they secure access to these reports to only their internal customer service agents?

   By utilizing the Power BI service, they can create a Content Pack that contains only the desired Dashboards, Reports, and Datasets and restrict access to those Groups in Azure Active Directory to which the customer service agents belong.

   ![In the Visualizing Bulk Delay Predictions diagram, Flight Delays Web Portal has an arrow labeled "1. Load Power BI embedded report," pointing to a Power BI icon labeled Visualize Delay Predictions on a Map. An arrow labeled "2. Report uses Power BI Direct Query against source and caches it," points from the Power BI icon to an Azure SQL Database icon.](media/visualizing-bulk-delay-predictions.png 'Visualizing Bulk Delay Predictions diagram')

3. MT wants a way to monitor their data pipeline, including ETL, data preparation, and model training activities. How can they capture and visualize metrics to watch for problems such as performance bottlenecks? How can they quickly access the logs from a single location?

   MT should use Azure Monitor to collect, analyze, and act on telemetry from [Azure Data Factory](https://docs.microsoft.com/azure/data-factory/monitor-using-azure-monitor), [Azure Databricks](https://docs.microsoft.com/azure/architecture/databricks-monitoring/), [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/monitor-azure-machine-learning), and [AKS](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-overview). Configuring these services to send application logs and other telemetry to Azure Monitor enables MT's operators to monitor the end-to-end data pipeline, including model training and deployments, from a central location.

   - Azure Data Factory:
     - At a glance summary of data factory pipeline, activity, and trigger runs
     - Ability to drill into data factory activity runs by type
     - Summary of data factory top pipeline, activity errors
   - Azure Databricks:
     - View application logs and metrics stored in a Log Analytics workspace
     - Deploy Grafana in a VM to visualize time series metrics stored in Log Analytics
     - Find and troubleshoot performance bottlenecks
     - Capture and query service metrics
     - Monitor and create alerts for model deployment failures
   - Azure Monitor for containers (AKS/ACI monitoring):
     - Collect and view performance data (memory and processor metrics) to identify resource bottlenecks
     - Understand the behavior of the cluster under average and heaviest loads

## Checklist of preferred objection handling

1. We have heard that creating a machine learning model takes a month to build and another 2-3 months to operationalize to be useable from our production systems. Is this true?

   This is true in the traditional process of creating machine learning models. The data scientist creates a model (e.g., in R) and then hands it over to developers who translate it into Java or C\#---which can take months to get the translation correct and performant. With Azure Machine Learning service, you can deploy Docker-based container images with a single command to Azure Kubernetes Service managed by the Machine Learning service workspace. During the process, a REST API for the deployed model is generated along with a swagger document. This makes it very easy for clients to consume the model.

2. Once our model is operationalized, how do we retrain and redeploy it? Will this process break clients currently accessing the deployed model?

   Azure Databricks allows you to retrain your models using notebooks.  Then, you can use the model deployment command to deploy an updated version of the model. As part of the data science workflow, you recreate the model in your experimentation environment. Then, you register the model with the Azure Databricks model registry. Updates are performed using a function call. This call replaces the active version of a model without changing the API URL or the key. The applications consuming the model continue to work without any code change and start getting better predictions using the new model.

3. Can we query flat files in the file system using SQL?

   Yes. There are many options for using a SQL syntax to query files in Blob storage, such as Azure Databricks, SQL Data Warehouse, and HDInsight with Spark SQL or Hive.

4. Does Azure provide anything that would speed up querying (and exploration) files in Hadoop Distributed File Systems (HDFS)?

   Yes. Hive on HDInsight provides the ability to create indices on flat-file content, as does SQL Data Warehouse. Azure Databricks provides a distributed in-memory cache that can aid exploratory queries that often repeat queries against the same subsets of data.

5. Does Azure provide any tools for visualizing our data? Ideally, access to these could be managed with Active Directory.

   Power BI is available as a service and provides tools for creating both dashboards and reports whose access can be restricted by group membership in Azure Active Directory.

6. Can we use Azure Active Directory accounts for our users? If so, can we restrict who can access Azure Databricks when they can access it, require two-factor authentication, and restrict access if there is suspicious activity on their account?

   Azure Databricks is automatically integrated with Azure Active Directory (Azure AD). This allows you to control access to your workspace and allows users to sign in with their Microsoft or organizational account. Since users can sign in from various devices from anywhere, simply focusing on who can access a resource is not sufficient anymore. Azure AD conditional access helps address this requirement by implementing automated access control decisions based on conditions. With conditional access, you can restrict sign-ins to your corporate network, require multi-factor authentication, restrict hours in which users can log in, and restrict access if there is a detected sign-in risk.

   To enable conditional access for Azure Databricks, perform the following steps:

   1. In the Azure Portal, open the **Azure Active Directory** service.

   2. Select **Conditional access** in the SECURITY section.

   3. Select **New policy** to create a new conditional access policy.

   4. In **Cloud apps**, click **Select apps**, and then search for the application ID `2ff814a6-3304-4ab8-85cb-cd0e6f879c1d`. Select **AzureDatabricks**.

      ![Selecting AzureDatabricks under Cloud Apps.](media/azure-databricks-conditional-access.png 'Cloud Apps')

   5. Fill in the other settings according to your desired conditional access configuration.

7. Is Azure Databricks our only option for running SQL on Hadoop solutions in Azure?

   No. Azure HDInsight provides an option for running hosted Spark clusters as well. Within HDInsight, you can provision hosted clusters based on Hive, HBase, and others. Hortonworks, Cloudera, and MapR provide fully supported distributions in the Azure Marketplace and provide environments for running HiveQL Spark SQL in Azure.

8. We have heard of Azure Data Lake, but we are not clear about whether this is currently a good fit for our PoC solution or whether we should be using it for interactive analysis of our data

   There are two offerings related to Azure Data Lake that should be called out separately. Azure Data Lake Analytics provides a distributed querying engine whose costs are based solely on the number of resources used in the query, the amount of time those resources were made available to the query, and a nominal job completion charge. In its current form, Azure Data Lake Analytics supports only Batch workloads, so it is not the right tool for interactive analytic workloads.

   Azure Data Lake Storage Gen2 provides the ability to store an unlimited number of items, each of unlimited size, having no upper limit on the total storage capacity. This makes it more capable than Azure Blob Storage since it removes the 2 PB account size limit, the page blob size limit, and the block blob limit imposed by Blob storage. However, given MT's low data growth projection, this is not a concern for them.

   Another differentiator between the two storage services is that Blob Storage uses a flat storage schema with path names as part of the bob file names. This makes the WASB driver Spark uses to connect to Blob Storage for HDFS access do extra work to map file structures to blob paths. This can lead to degraded performance in cases of high volume and many small files. In contrast, ADLS Gen2 has hierarchical storage that provides granular POSIX permissions and Access Control Lists (ACLs). This provides greater flexibility in controlling access to files, and Spark uses the newer ABFS driver, which can provide higher performance. However, MT's requirements do not include fine-grained permissions and hierarchical storage, and the size and velocity of their data will mean that they would likely never notice a significant performance difference between the two options.

   Given their requirements and comparison of services, Blob Storage is recommended as it meets MT's requirements and is a more cost-effective solution.

9. We are hiring a data scientist who prefers to use MLflow to track model training run metrics and artifacts. Can the proposed Azure-based solution support this library?

   Yes, MLflow is supported automatically in Azure Databricks. MLflow is an open-source project that data scientists and developers use to instrument their machine learning code to track metrics and artifacts. MLflow keeps training code cloud-agnostic. The data scientist can continue to use MLflow while integrating with either Azure Databricks or Azure Machine Learning to manage and track runs and artifacts securely.

## Customer quote (to be read back to the attendees at the end)

"We are flying into the future with Azure, helping our customers more aggressively schedule their travel and optimize their non-travel time."

Jack Tradewinds, CIO of Margie's Travel
