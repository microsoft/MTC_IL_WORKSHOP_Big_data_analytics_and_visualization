# Databricks notebook source
# MAGIC %md 
# MAGIC ##### Access ADLS Using SAS token
# MAGIC 
# MAGIC Note that the recommended way to access ADLS from Databricks is by using AAD Service Principal and the backed by Azure Key Vault Databricks Secret Scope.
# MAGIC 
# MAGIC Here for simplicity we use SAS token.
# MAGIC 
# MAGIC Note: Replace the storage account and coontainer with your names  

# COMMAND ----------

storage_account='asastoremcw303474'
container_name = 'labs-303474'
data_folder_name = 'FlightsDelays'

# COMMAND ----------

spark.conf.set("fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net", "SAS")
spark.conf.set("fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider")
spark.conf.set("fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net", "<SAS token replace by your value>")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### ABFS protocol
# MAGIC 
# MAGIC ABFS protocol (Azure Blob File System) - Azure Blob storage driver for Hadoop required by Spark.
# MAGIC ABFS is part of Apache Hadoop and is included in many of the commercial distributions of Hadoop. It's a recommended protocol today to use when working with ADLS v2
# MAGIC 
# MAGIC The objects in ADLS are represented as URIs with the following URI schema:
# MAGIC 
# MAGIC _abfs[s]://container_name@account_name.dfs.core.windows.net/<path>/<path>/<file_name>_
# MAGIC   
# MAGIC If you add an **_'s'_** at the end (abfss) then the ABFS Hadoop client driver will ALWAYS use Transport Layer Security (TLS) irrespective of the authentication method chosen.

# COMMAND ----------

# MAGIC %md
# MAGIC ##### dbutils
# MAGIC 
# MAGIC Databricks utilities tool
# MAGIC 
# MAGIC You can do many opertations with dbutils, for more details see [dbutils](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/databricks-utils)
# MAGIC 
# MAGIC List the content of ADLS folder _FlightsDelays_ in container _labs-303474_:

# COMMAND ----------

dbutils.fs.ls( f"abfss://{container_name}@{storage_account}.dfs.core.windows.net/FlightsDelays/")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Initial data exploration
# MAGIC 
# MAGIC Run sparkSQL directly on the files in ADLS.
# MAGIC 
# MAGIC Later we'll create schemas and tables 

# COMMAND ----------

# MAGIC %sql
# MAGIC --select * from csv.`abfss://labs-303474@asastoremcw303474.dfs.core.windows.net/FlightsDelays/FlightDelaysWithAirportCodes.csv`

# COMMAND ----------

# MAGIC %md 
# MAGIC Databricks provides built-in Spark dataframe `_sqldf` whe running sparkSQL. We can reuse it in the following operations

# COMMAND ----------

#buil-in _sqldf dataframe created automatically by databricks
#_sqldf.count()

# COMMAND ----------

# MAGIC %md
# MAGIC Let's check other two tables

# COMMAND ----------

# MAGIC %sql
# MAGIC --select * from csv.`abfss://labs-303474@asastoremcw303474.dfs.core.windows.net/FlightsDelays/AirportCodeLocationLookupClean.csv`

# COMMAND ----------

# MAGIC %sql
# MAGIC --select * from csv.`abfss://labs-303474@asastoremcw303474.dfs.core.windows.net/FlightsDelays/FlightWeatherWithAirportCode.csv`

# COMMAND ----------

# MAGIC %md
# MAGIC Load file to Spark DataFrame (pySpark)

# COMMAND ----------

df = spark.read.csv(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/FlightsDelays/FlightDelaysWithAirportCodes.csv', header=True)

# COMMAND ----------

display(df)

# COMMAND ----------

# MAGIC %md 
# MAGIC Run Spark operations of DataFrame we've just created

# COMMAND ----------

# Print file schema in dataframe 
df.printSchema()

# COMMAND ----------

from pyspark.sql import functions as F
# Group the data by the Month column and count the number of rows in each group ans sort it
res = df.groupBy("Month").agg(F.count("*").alias("Count")).sort(F.desc("Count"))
display(res)

# COMMAND ----------

# count delays per Origin airport
from pyspark.sql.types import IntegerType
#Cast DepDel5 to Int 
df = df.withColumn("DepDel15", df["DepDel15"].cast(IntegerType()))
#Calculate number of delays per origin airport
delay_counts = df.groupBy("OriginAirportName").agg({"DepDel15": "sum"})
#Rename autogenerated column to human readable
delay_counts = delay_counts.withColumnRenamed("sum(DepDel15)", "DelayCount")
display(delay_counts)

# COMMAND ----------

# Count all cases of double delay: in Origin and Destination 
from pyspark.sql import functions as F

# Filter only the rows where DepDel15 is equal to 1 (delayed flights)
delayed_flights = df.filter(df["DepDel15"] == "1")

# Group the data by OriginAirportCode and DestAirportName and count the number of delayed flights
result = delayed_flights.groupBy("OriginAirportName", "DestAirportName").agg(F.count("DepDel15").alias("Delay Count"))

# Show the result
display(result)

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Data Quality research

# COMMAND ----------

# MAGIC %md Check the number of null values in DepDel15, this colimn states the departure delay of at least      in 15 min.
# MAGIC 
# MAGIC If we want to use this field for delay prediction we should fix those records with null values. 

# COMMAND ----------

df = spark.read.csv(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/FlightsDelays/FlightDelaysWithAirportCodes.csv', header=True)
display(df)

# COMMAND ----------

#check percentage of missing values for our target field: DepDel15
from pyspark.sql.functions import col
percentage_nulls_in_DepDel15 = df.filter(col("DepDel15").isNull() ).count() / df.count() * 100
print (f"{percentage_nulls_in_DepDel15} % null values in DepDel15 column") 

# COMMAND ----------

display(df.select('DepDel15').distinct())

# COMMAND ----------

#check if there is not valid values beside nulls
from pyspark.sql.functions import col
for col in df.columns:
    display(df.select(col).distinct())
    

# COMMAND ----------

#let's do it simpler
dbutils.data.summarize(df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### `Excersise` 
# MAGIC Run dbutils.summarize for FlightWeatherWithAirportCode file
# MAGIC 
# MAGIC Check which columns have null values and must be fixed 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Create schema and tables
# MAGIC 
# MAGIC After we explored the data we create schemas and tables in order to work with the data as we get used to work with regular data bases

# COMMAND ----------

# MAGIC %sql 
# MAGIC show schemas

# COMMAND ----------

# MAGIC %sql
# MAGIC create schema flights

# COMMAND ----------

# MAGIC %sql
# MAGIC describe schema flights

# COMMAND ----------

dbutils.fs.ls("dbfs:/user/hive/warehouse/flights.db")

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Create external table
# MAGIC We create a table in `flights` schema, which we created previously.
# MAGIC 
# MAGIC Notice using ***_header_*** option, which allows to define columns names from the header

# COMMAND ----------

# MAGIC %sql
# MAGIC use flights

# COMMAND ----------

# MAGIC %sql
# MAGIC create table flights_delays_external
# MAGIC using csv options (
# MAGIC   path = 'abfss://labs-303474@asastoremcw303474.dfs.core.windows.net/FlightsDelays/FlightDelaysWithAirportCodes.csv',
# MAGIC   header = "true");
# MAGIC   

# COMMAND ----------

# MAGIC %sql
# MAGIC show tables

# COMMAND ----------

# MAGIC %sql 
# MAGIC describe extended flights_delays_external

# COMMAND ----------

# MAGIC %md
# MAGIC Look at table `Location` property. 
# MAGIC 
# MAGIC Where does it point to?
# MAGIC 
# MAGIC Pay attention that this table is ***External*** and the table format is CSV. Since it's an extrernal table, the location is the original one: the ADLS container.
# MAGIC 
# MAGIC Also you can see that there is no history for non-delta tables.
# MAGIC 
# MAGIC The history is supported by Delta format.

# COMMAND ----------

# MAGIC %sql
# MAGIC describe history flights_delays_external

# COMMAND ----------

# MAGIC %md Now we can work with this table as we get used working with regular tables...

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from flights_delays_external limit 10

# COMMAND ----------

# MAGIC %sql
# MAGIC select DepDel15, count(*) as number_of_delays
# MAGIC from flights_delays_external
# MAGIC group by DepDel15

# COMMAND ----------

# MAGIC %sql
# MAGIC select DepDel15, count(DepDel15) as number_of_delays
# MAGIC from flights_delays_external
# MAGIC group by DepDel15

# COMMAND ----------

# MAGIC %md
# MAGIC Why results of previous two queries are different?

# COMMAND ----------

# MAGIC %sql
# MAGIC select distinct (DepDel15)
# MAGIC from flights_delays_external

# COMMAND ----------

# MAGIC %sql
# MAGIC select count (distinct (DepDel15))
# MAGIC from flights_delays_external

# COMMAND ----------

# MAGIC %sql 
# MAGIC describe table flights_delays_external

# COMMAND ----------

# MAGIC %md
# MAGIC What's wrong with this table defintion?
# MAGIC 
# MAGIC Since the original data is in CSV format, Spark can not infer precisely the correct fields types (by default). 
# MAGIC 
# MAGIC You can enforce schema inference also for CSV, JSON formats by setting format option : `inferSchema=true`, see below examples
# MAGIC 
# MAGIC Howver the better way is to create table and define the types strictly.. 

# COMMAND ----------

# MAGIC %sql drop table flights_delays

# COMMAND ----------

# MAGIC %sql create table flights_delays (
# MAGIC   year int,
# MAGIC   month int,
# MAGIC   dayofmonth int,
# MAGIC   dayofweek int,
# MAGIC   carrier string,
# MAGIC   crsdeptime string,
# MAGIC   depdelay int,
# MAGIC   depdel15 int,
# MAGIC   crsarrTime string,
# MAGIC   arrdelay int,
# MAGIC   arrdel15 int,
# MAGIC   canceled smallint,
# MAGIC   originairportcode string,
# MAGIC   originairportname string,
# MAGIC   originlatitude float,
# MAGIC   originlongitude float,
# MAGIC   destairportcode string,
# MAGIC   destairportname string,
# MAGIC   destlatitude float,
# MAGIC   destlongitude float
# MAGIC ) using csv options (
# MAGIC   path = 'abfss://labs-303474@asastoremcw303474.dfs.core.windows.net/FlightsDelays/FlightDelaysWithAirportCodes.csv',
# MAGIC   header = "true"
# MAGIC );

# COMMAND ----------

# MAGIC %sql describe extended  flights_delays

# COMMAND ----------

# MAGIC %md
# MAGIC #### CTEs
# MAGIC 
# MAGIC Spark supports CTEs - Common Table Expessions.
# MAGIC 
# MAGIC CTEs defines a temporary result set that can be refernced multiple times in other queries 
# MAGIC 
# MAGIC In order to define CTE - you use `WITH` clause

# COMMAND ----------

# MAGIC %sql
# MAGIC select 
# MAGIC   (
# MAGIC     with flights_dep_delays (origin_airport, dep_total_delays) as (
# MAGIC       select
# MAGIC         originairportname,
# MAGIC         sum(depdel15) as origindelay15min
# MAGIC       from
# MAGIC         flights_delays
# MAGIC       group by
# MAGIC         originairportname
# MAGIC     )
# MAGIC     select
# MAGIC       max(dep_total_delays) as total_dep_delays
# MAGIC     from
# MAGIC       flights_dep_delays
# MAGIC   )

# COMMAND ----------

# MAGIC %sql with flights_dep_delays (origin_airport, dep_total_delays) as (
# MAGIC   select
# MAGIC     originairportname,
# MAGIC     sum(depdel15) as origindelay15min
# MAGIC   from
# MAGIC     flights_delays
# MAGIC   group by
# MAGIC     originairportname
# MAGIC )
# MAGIC select
# MAGIC   origin_airport,
# MAGIC   dep_total_delays
# MAGIC from
# MAGIC   flights_dep_delays
# MAGIC order by dep_total_delays desc

# COMMAND ----------

# MAGIC %md
# MAGIC Let's create other two table with explicit field types defintion

# COMMAND ----------

# MAGIC %sql -- Task TODO create external table: airports_codes_locations
# MAGIC create table airports_codes_locations(
# MAGIC   airportid int,
# MAGIC   airport string,
# MAGIC   displayairportname string,
# MAGIC   latitude float,
# MAGIC   longitude float
# MAGIC ) using csv options (
# MAGIC   header = "true",
# MAGIC   path = "abfss://labs-303474@asastoremcw303474.dfs.core.windows.net/FlightsDelays/AirportCodeLocationLookupClean.csv"
# MAGIC )

# COMMAND ----------

# MAGIC %sql create table flights_with_weather(
# MAGIC   year int,
# MAGIC   month int,
# MAGIC   day int,
# MAGIC   time int,
# MAGIC   timezone int,
# MAGIC   skycondition string,
# MAGIC   visibility float,
# MAGIC   weathertype string,
# MAGIC   drybulbfarenheit float,
# MAGIC   drybulbcelsius float,
# MAGIC   wetbulbfarenheit float,
# MAGIC   wetbulbcelsius float,
# MAGIC   dewpointfarenheit float,
# MAGIC   dewpointcelsius float,
# MAGIC   relativehumidity int,
# MAGIC   windspeed int,
# MAGIC   winddirection int,
# MAGIC   valueforwindcharacter int,
# MAGIC   stationpressure float,
# MAGIC   pressuretendency int,
# MAGIC   pressurechange int,
# MAGIC   sealevelpressure float,
# MAGIC   recordtype string,
# MAGIC   hourlyprecip string,
# MAGIC   altimeter float,
# MAGIC   airportcode string,
# MAGIC   displayairportname string,
# MAGIC   latitude float,
# MAGIC   longitude float
# MAGIC ) using csv options (
# MAGIC   header = "true",
# MAGIC   path = "abfss://labs-303474@asastoremcw303474.dfs.core.windows.net/FlightsDelays/FlightWeatherWithAirportCode.csv"
# MAGIC )

# COMMAND ----------

# MAGIC %sql
# MAGIC show tables

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Create Table As Select (CTAS)
# MAGIC 
# MAGIC Create Delta Lake tables with CTAS

# COMMAND ----------

# MAGIC %sql create
# MAGIC or replace table flights_with_weather_delta as
# MAGIC select
# MAGIC   *
# MAGIC from
# MAGIC   flights_with_weather

# COMMAND ----------

# MAGIC %sql
# MAGIC describe extended flights_with_weather_delta

# COMMAND ----------

# MAGIC %md
# MAGIC Now the table type is `Managed`.  By default the tables are created in Delta format.
# MAGIC 
# MAGIC For managed tables, the data is copied from original location (ADLS container) to the `schema` location in dbfs (Databricks file system). 
# MAGIC 
# MAGIC The Databricks File System (DBFS) is a distributed file system mounted into an Azure Databricks workspace and available on Azure Databricks clusters. 
# MAGIC 
# MAGIC If you delete the table all this data will be deleted as well (not the case with `External` tables)

# COMMAND ----------

dbutils.fs.ls("dbfs:/user/hive/warehouse/flights.db/flights_with_weather_delta/")

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ##### `Exercise` 
# MAGIC 
# MAGIC Research a little bit Delta format, which folders, files are created for Delta

# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC Note that CTAS syntax doesn't support schema definition (it infers the schema from query results)
# MAGIC 
# MAGIC The recommended way to overcome it is to create a temp view, define schema for it and then run CTAS (similar as we did before but we used intermediate tables rather than temp views)
# MAGIC 
# MAGIC Temp view exists only during Spark Session.
# MAGIC 
# MAGIC For example

# COMMAND ----------

# MAGIC %sql create
# MAGIC or replace temp view flights_with_weather_temp_view(
# MAGIC   year int,
# MAGIC   month int,
# MAGIC   day int,
# MAGIC   time int,
# MAGIC   timezone int,
# MAGIC   skycondition string,
# MAGIC   visibility string,
# MAGIC   weathertype string,
# MAGIC   drybulbfarenheit string,
# MAGIC   drybulbcelsius float,
# MAGIC   wetbulbfarenheit float,
# MAGIC   wetbulbcelsius float,
# MAGIC   dewpointfarenheit float,
# MAGIC   dewpointcelsius float,
# MAGIC   relativehumidity int,
# MAGIC   windspeed int,
# MAGIC   winddirection int,
# MAGIC   valueforwindcharacter int,
# MAGIC   stationpressure float,
# MAGIC   pressuretendency int,
# MAGIC   pressurechange int,
# MAGIC   sealevelpressure float,
# MAGIC   recordtype string,
# MAGIC   hourlyprecip string,
# MAGIC   altimeter float,
# MAGIC   airportcode string,
# MAGIC   displayairportname string,
# MAGIC   latitude float,
# MAGIC   longitude float
# MAGIC ) using csv options (
# MAGIC   header = "true",
# MAGIC   path = "abfss://labs-303474@asastoremcw303474.dfs.core.windows.net/FlightsDelays/FlightWeatherWithAirportCode.csv"
# MAGIC )

# COMMAND ----------

# MAGIC %md 
# MAGIC Now run CTAS, which creates managed table from temp view

# COMMAND ----------

# MAGIC %sql create
# MAGIC or replace table flights_with_weather_delta_from_view as
# MAGIC select
# MAGIC   *
# MAGIC from
# MAGIC   flights_with_weather_temp_view

# COMMAND ----------

# MAGIC %sql describe extended flights_with_weather_delta_from_view

# COMMAND ----------

# MAGIC %md
# MAGIC ##### `Excersise` 
# MAGIC 
# MAGIC Create 2 additional delta tables: **airports_codes_locations_delta** and **flights_delays_delta** using CTAS techique.

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC #### Enriching data with additional meta-data

# COMMAND ----------

# MAGIC %md 
# MAGIC Using built-in `current_timestamp()` function
# MAGIC 
# MAGIC There are many others, more details [databricks built-in functions](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/current_timestamp)

# COMMAND ----------

# MAGIC %sql create
# MAGIC or replace table flights_with_weather_delta_from_view as
# MAGIC select
# MAGIC   current_timestamp() as ingestiontime,
# MAGIC   *
# MAGIC from
# MAGIC   flights_with_weather_temp_view

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from flights_with_weather_delta_from_view limit 10

# COMMAND ----------

# MAGIC %md
# MAGIC Very often there is a need to add data incrementally to the existing tables
# MAGIC There are few ways to achieve it in Spark by using one of those:
# MAGIC 1. `INSERT INTO`
# MAGIC 2. `COPY INTO`
# MAGIC 3. `MERGE INTO`

# COMMAND ----------

# MAGIC %md 
# MAGIC 
# MAGIC let's simulate incremental load flow
# MAGIC first we create a table: `incremental_flights_with_weather_data` which we want to copy incrementally to the original table:`flights_with_weather_delta_from_view`

# COMMAND ----------

# MAGIC %sql create or replace temp view incremental_flights_with_weather_data as
# MAGIC select
# MAGIC   *
# MAGIC from
# MAGIC   flights_with_weather_delta_from_view
# MAGIC where
# MAGIC   airportcode == "SJU"

# COMMAND ----------

# MAGIC %sql
# MAGIC select count(*) from flights_with_weather_delta_from_view

# COMMAND ----------

# MAGIC %md
# MAGIC INSERT INTO

# COMMAND ----------

# MAGIC %sql
# MAGIC insert into
# MAGIC   flights_with_weather_delta_from_view (
# MAGIC     select
# MAGIC       *
# MAGIC     from
# MAGIC       incremental_flights_with_weather_data
# MAGIC   )

# COMMAND ----------

# MAGIC %md
# MAGIC Run the previous `INSERT INTO` one more time, you will succeed, effectively duplicating the data. The meaning is `INSERT INTO` operation **is not idempotent**.

# COMMAND ----------

# MAGIC %sql
# MAGIC select count(*) from flights_with_weather_delta_from_view

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ##### INSERT INTO vs. COPY INTO
# MAGIC 
# MAGIC Important: COPY INTO **is idempotent operation** and so recommended one for the incremental data load. 
# MAGIC 
# MAGIC It will help you to avoid data duplication in the target tables.
# MAGIC 
# MAGIC Let's see it in action

# COMMAND ----------

# MAGIC %sql drop table airports_codes_locations_stg

# COMMAND ----------

# MAGIC %sql create table airports_codes_locations_stg(
# MAGIC   airportid int,
# MAGIC   airport string,
# MAGIC   displayairportname string,
# MAGIC   latitude double,
# MAGIC   longitude double
# MAGIC ) 

# COMMAND ----------

# MAGIC %sql copy into airports_codes_locations_stg
# MAGIC from
# MAGIC   "abfss://labs-303474@asastoremcw303474.dfs.core.windows.net/FlightsDelays/AirportCodeLocationLookupClean.csv" 
# MAGIC     fileformat = CSV 
# MAGIC     format_options (
# MAGIC     "mergeSchema" = "true",
# MAGIC     "header" = "true",
# MAGIC     "inferSchema" = "true"
# MAGIC   ) copy_options ("mergeSchema" = "true")

# COMMAND ----------

# MAGIC %sql select count(*) from airports_codes_locations_stg

# COMMAND ----------

# MAGIC %md 
# MAGIC Run once again the previous `COPY INTO`- zero records will be inserted. 
# MAGIC 
# MAGIC You can see the second running for the same data afffects zero rows and inserts zero.  In other words `INSERT INTO` is idempotent operation 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### MERGE INTO

# COMMAND ----------

# MAGIC %sql
# MAGIC show tables

# COMMAND ----------

# MAGIC %sql create or replace temp view incremental_flights_with_weather_data_atl as
# MAGIC select
# MAGIC  *
# MAGIC from
# MAGIC   flights_with_weather_delta_from_view
# MAGIC where
# MAGIC   airportcode == "ATL"

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO flights_with_weather_delta_from_view a
# MAGIC USING incremental_flights_with_weather_data_atl b
# MAGIC ON a.airportcode = b.airportcode
# MAGIC WHEN NOT MATCHED THEN INSERT *

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Delta format
# MAGIC 
# MAGIC Let's explore Delta Lake format
# MAGIC 
# MAGIC Delta Lake provides ACID transactions, hstory, time travel, schema enforcement, perfromance optimizations   

# COMMAND ----------

# MAGIC %sql drop view incremental_flights_with_weather_data_temp_view

# COMMAND ----------

# MAGIC %sql
# MAGIC create temp view incremental_flights_with_weather_data_temp_view as
# MAGIC select
# MAGIC   *,
# MAGIC   1 as dirty_column_flag
# MAGIC from
# MAGIC   flights_with_weather_delta_from_view
# MAGIC where
# MAGIC   airportcode == "SJU"

# COMMAND ----------

# MAGIC %md
# MAGIC Note that we changed the schema - the new column `dirty_column` has been added
# MAGIC 
# MAGIC That's the one of great Delta features: schema enforcement. 
# MAGIC The default behaviour is to fail. 
# MAGIC You can enable the insert by allowing ***schema merge***

# COMMAND ----------

# MAGIC %sql
# MAGIC insert into
# MAGIC   flights_with_weather_delta_from_view (
# MAGIC     select
# MAGIC       *
# MAGIC     from
# MAGIC       incremental_flights_with_weather_data_temp_view
# MAGIC   )

# COMMAND ----------

# MAGIC %md Schema enforcement in action
# MAGIC In this specific case you can enforce merge, since adding new field doesn't break a schema (echema evolution)
# MAGIC 
# MAGIC Set Spark parameter below, to enforce schema merge and run the `INSERT INTO` once again

# COMMAND ----------

spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled",  "true")

# COMMAND ----------

# MAGIC %md
# MAGIC Run again `INSERT INTO`

# COMMAND ----------

# MAGIC %sql 
# MAGIC describe history flights_with_weather_delta_from_view

# COMMAND ----------

# MAGIC %sql
# MAGIC select count(*) from flights_with_weather_delta_from_view

# COMMAND ----------

# MAGIC %md 
# MAGIC Transactional history and Time Travel with Delta

# COMMAND ----------

# MAGIC %sql
# MAGIC describe history flights_with_weather_delta_from_view

# COMMAND ----------

# MAGIC %sql
# MAGIC select count(*) from flights_with_weather_delta_from_view version as of  0

# COMMAND ----------

# MAGIC %md
# MAGIC Roolback to the table previous version with Delta

# COMMAND ----------

# MAGIC %sql
# MAGIC restore table flights_with_weather_delta_from_view version as of 0

# COMMAND ----------

# MAGIC %sql 
# MAGIC describe history flights_with_weather_delta_from_view
