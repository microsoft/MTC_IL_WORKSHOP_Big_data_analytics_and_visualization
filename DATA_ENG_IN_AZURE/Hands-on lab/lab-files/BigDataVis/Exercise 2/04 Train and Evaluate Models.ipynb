{"cells":[{"cell_type":"markdown","source":["# Train the model"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3c0e1fb7-2236-4273-b586-8efed5b569cd","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Margie's Travel wants to build a model to predict if a departing flight will have a 15-minute or greater delay. In the historical data they have provided, the indicator for such a delay is found within the DepDel15 (where a value of 1 means delay, 0 means no delay). To create a model that predicts such a binary outcome, we can choose from the various Two-Class algorithms provided by Spark MLlib. For our purposes, we choose Decision Tree. This type of classification module needs to be first trained on sample data that includes the features important to making a prediction and must also include the actual historical outcome for those features. \n\nThe typical pattern is to split the historical data so a portion is shown to the model for training purposes, and another portion is reserved to test just how well the trained model performs against examples it has not seen before."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e7adc10-d900-4e94-aa3a-6f124b5fc762","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["To start, let's import the Python libraries and modules we will use in this notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ab0acfd-df62-4f1d-a31e-13062acf2d0f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nfrom pyspark.sql.functions import array, col, lit"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49e26fb4-fc9f-4e1c-971b-292b0f669dd7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Load the cleaned flight and weather data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81d783d9-5330-4efb-866b-c3a70386644b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Load the data from the global table."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"573da089-45f7-43ce-adaf-55a8a5c346ce","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nselect \n    OriginAirportCode, cast(Month as int) Month, cast(DayofMonth as int) DayofMonth, CRSDepHour, cast(DayOfWeek as int) DayOfWeek, Carrier, \n    DestAirportCode, DepDel15, WindSpeed, SeaLevelPressure, HourlyPrecip \nfrom\n    flight_delays_with_weather\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0699cbd1-c074-418e-b301-126f12d5f482","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dfDelays = _sqldf\ncols = dfDelays.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"70ea4625-d04e-4327-b5ba-f7a06a204bd3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(dfDelays)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5d720ada-96bb-4091-b689-4401471017ce","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can get a sense of which origin and destination airports suffer the most delays by querying against the table and displaying the output as an area chart. We've already configured the chart's settings so you should see a nice visual when you run the below command. If it displays in a table instead, just select the area chart option below the table."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a630a694-212e-47d4-bbcf-47074348fe15","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nselect OriginAirportCode, DestAirportCode, count(DepDel15)\nfrom flight_delays_with_weather where DepDel15 = 1\ngroup by OriginAirportCode, DestAirportCode\nORDER BY count(DepDel15) desc"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"b5f38c99-ff2f-4387-8d0f-7cb691b83775","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Sampling the data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1274ed2c-bbde-4c23-8bf1-22f54cee33c5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["To begin, let's evaluate the data to compare the flights that are delayed (DepDel15) to those that are not. What we're looking for is whether one group has a much higher count than the other."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d91fe98c-76eb-44d9-93f2-6c85d110d407","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dfDelays.groupBy(\"DepDel15\").count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"659fe34d-9bfc-4a3d-b61c-4923250ec199","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Judging by the delay counts, there are almost four times as many non-delayed records as there are delayed.\n\nWe want to ensure our model is sensitive to the delayed samples. To do this, we can use stratified sampling provided by the `sampleBy()` function. First we create fractions of each sample type to be returned. In our case, we want to keep all instances of delayed (value of 1) and downsample the not delayed instances to 30%."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89083449-e71a-4f70-8235-9855968ba79e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["fractions = {0: .30, 1: 1.0}\ntrainingSample = dfDelays.sampleBy(\"DepDel15\", fractions, 36)\ntrainingSample.groupBy(\"DepDel15\").count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b8a99665-15d4-469a-8ce8-549b78719102","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can see that the number of delayed and not delayed instances are now much closer to each other. This should result in a better-trained model."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b840e596-d67e-4df3-878b-93dcdc7630d1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Select an algorithm and transform features"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"44570f9c-4f8c-4507-b39f-692ab3f58606","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Because we are trying to predict a binary label (flight delayed or not delayed), we need to use binary classification. For this, we will be using the [Decision Tree](https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier) classifier algorithm provided by the Spark MLlib library. We will also be using the [Pipelines API](https://spark.apache.org/docs/latest/ml-guide.html) to put our data through all of the required feature transformations in a single call. The Pipelines API provides higher-level API built on top of DataFrames for constructing ML pipelines."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3e89df5-3bba-4f77-81f0-22085ddc4ef7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["In the data cleaning phase, we identified the important features that most contribute to the classification. The `flight_delays_with_weather` is the result of the data preparation and feature identification process. The features are:\n\n| OriginAirportCode | Month | DayofMonth | CRSDepHour | DayOfWeek | Carrier | DestAirportCode | WindSpeed | SeaLevelPressure | HourlyPrecip |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| LGA | 5 | 2 | 13 | 4 | MQ | ORD | 6 | 29.8 | 0.05 |\n\nWe also have a label named `DepDelay15` which equals 0 if no delay, and 1 if there was a delay.\n\nAs you can see, this dataset contains nominal variables like OriginAirportCode (LGA, MCO, ORD, ATL, etc). In order for the machine learning algorithm to use these nominal variables, they need to be transformed and put into Feature Vectors, or vectors of numbers representing the value for each feature.\n\nFor simplicity's sake, we will use One-Hot Encoding to convert all categorical variables into binary vectors. We will use a combination of StringIndexer and OneHotEncoderEstimator to convert the categorical variables. The `OneHotEncoderEstimator` will return a `SparseVector`.\n\nSince we will have more than 1 stage of feature transformations, we use a Pipeline to tie the stages together. This simplifies our code."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f7c2354f-afe8-46f5-a4f2-585e843d7e21","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["The ML package needs the label and feature vector to be added as columns to the input dataframe. We set up a pipeline to pass the data through transformers in order to extract the features and label. We index each categorical column using the `StringIndexer` to a column of number indices, then convert the indexed categories into one-hot encoded variables with at most a single one-value. These binary vectors are appended to the end of each row. Encoding categorical features allows decision trees to treat categorical features appropriately, improving performance. We then use the `StringIndexer` to encode our labels to label indices."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1cd7f165-c673-48bf-ae54-e7520dfa2e7d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["categoricalColumns = [\"OriginAirportCode\", \"Carrier\", \"DestAirportCode\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoderEstimator to convert categorical variables into binary SparseVectors\n    encoder = OneHotEncoder(dropLast=False, inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"DepDel15\", outputCol=\"label\")\nstages += [label_stringIdx]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e09be5d-fb96-4068-9e36-f17bad545b24","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now we need to use the `VectorAssembler` to combine all the feature columns into a single vector column. This includes our numeric columns as well as the one-hot encoded binary vector columns."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"407890c9-0c28-4f5a-95db-aa54fc05ab6c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Transform all features into a vector using VectorAssembler\nnumericCols = [\"Month\", \"DayofMonth\", \"CRSDepHour\", \"DayOfWeek\", \"WindSpeed\", \"SeaLevelPressure\", \"HourlyPrecip\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2875427-b090-4e1c-b7e5-21a5160f2e10","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create and train the Decision Tree model"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bf335ec3-f829-4c8a-a735-86827634c30c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Before we can train our model, we need to randomly split our data into test and training sets. As is standard practice, we will allocate a larger portion (70%) for training. A seed is set for reproducibility, so the outcome is the same (barring any changes) each time this cell and subsequent cells are run.\n\nRemember to use our stratified sample (`trainingSample`)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c9c6422-600f-4be9-a30d-da92ae99d325","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = trainingSample.randomSplit([0.7, 0.3], seed=100)\n# We want to have two copies of the training and testing data, since the pipeline runs transformations and we want to run a couple different iterations\ntrainingData2 = trainingData\ntestData2 = testData\nprint(trainingData.count())\nprint(testData.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ff3e105-70f6-4989-bc38-cf4211811585","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Our pipeline is ready to be built and run, now that we've created all the transformation stages. We just have one last stage to add, which is the Decision Tree. Let's run the pipeline to put the data through all the feature transformations within a single call.\n\nCalling `pipeline.fit(trainingData)` will transform the test data and use it to train the Decision Tree model.\n\nWe will also use the MLflow library to track the details of this experiment, including testing results and the model we create."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"351ce9ab-e5bd-4ee5-970b-27e92a5ab274","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\nimport mlflow\nimport mlflow.spark\n\nmlflow.start_run()\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\nstages += [dt]\n\n# Create a Pipeline.\npipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(trainingData)\ntrainingData = pipelineModel.transform(trainingData)\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ntrainingData = trainingData.select(selectedcols)\ndisplay(trainingData)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5bcf3c1e-e740-4df5-ae16-7bf93f7a4561","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's make predictions on our test dataset using the `transform()`, which will only use the 'features' column. We'll display the prediction's schema afterward so you can see the three new prediction-related columns."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8a918f4b-20c2-4b1d-9f90-f89479396584","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Make predictions on test data using the Transformer.transform() method.\npredictions = pipelineModel.transform(testData)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d8371573-dd53-4586-9235-7cdccaa3d886","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["To properly train the model, we need to determine which parameter values of the decision tree produce the best model. A popular way to perform model selection is k-fold cross validation, where the data is randomly split into k partitions. Each partition is used once as the testing data set, while the rest are used for training. Models are then generated using the training sets and evaluated with the testing sets, resulting in k model performance measurements. The model parameters leading to the highest performance metric produce the best model.\n\nWe can use `BinaryClassificationEvaluator` to evaluate our model. We can set the required column names in `rawPredictionCol` and `labelCol` Param and the metric in `metricName` Param."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90b499f5-d8f5-41c8-9829-45a474a78b79","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Let's evaluate the Decision Tree model with `BinaryClassificationEvaluator`."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c833af1-8d4c-4978-ba57-65868ef03077","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nareaUnderRoc = evaluator.evaluate(predictions)\nmlflow.log_metric(\"Area Under ROC\", areaUnderRoc)\nareaUnderRoc"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7a33b3db-b7ce-4bc0-954e-a830de4528fb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finally, we will save the model to disk and end the first MLflow run."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"436e4af9-e33c-41a2-83de-4c2988902ebf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["mlflow.spark.log_model(pipelineModel, \"model\")\nmodelpath = \"/dbfs/mlflow/mtc/model-dtree\"\nmlflow.spark.save_model(pipelineModel, modelpath)\nmlflow.end_run()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82de744c-3ae1-407d-beef-760f39457861","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now we will try tuning the model with the `ParamGridBuilder` and the `CrossValidator`.\n\nAs we indicate 3 values for maxDepth and 3 values for maxBin, this grid will have 3 x 3 = 9 parameter settings for `CrossValidator` to choose from. We will create a 3-fold CrossValidator."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b712fb2-45fa-47da-840c-ce1cab98c62f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1, 2, 6, 10])\n             .addGrid(dt.maxBins, [20, 40, 80])\n             .build())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81ce8f45-9ff7-4789-b497-28b10011ff1c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run the cell below to create your 3-fold CrossValidator and use it to run cross validations. It can take **up to 5 minutes** to execute the cell.\n\nBecause we are training a new model, we will do this in another run of the same experiment.  That way, we can compare the cross-validated version to the original decision tree and choose the better model for deployment."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4d24e97-7c52-4ace-98d0-9eee2fcca32d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["mlflow.start_run()\n\n# Create 3-fold CrossValidator\ncv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n\n# Run cross validations (this can take several minutes to execute)\ncvModel = cv.fit(trainingData2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"38f2e8fd-06e8-4932-b780-81169112e525","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now let's create new predictions with which to measure the accuracy of our model."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a81887a-109e-4dc7-8c95-40cdd21b1a59","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["predictions = cvModel.transform(testData2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1cd32c2d-6452-4d47-bb49-4bb9f57fc9f1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We'll use the predictions to evaluate the best model. `cvModel` uses the best model found from the Cross Validation."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"56551233-96f3-45ec-968a-aac525da7358","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["areaUnderRoc = evaluator.evaluate(predictions)\nmlflow.log_metric(\"Area Under ROC\", areaUnderRoc)\nareaUnderRoc"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"09de1d1e-e7ca-4c1e-b536-85117a3aa19f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now let's view the best model's predictions and probabilities of each prediction class."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"22c1a8a7-9813-4e04-863e-91fa09e44a32","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["selected = predictions.select(\"label\", \"prediction\", \"probability\", \"OriginAirportCode\", \"DestAirportCode\")\ndisplay(selected)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"77362ece-54b5-4526-a6ff-ae2ca0f7111c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We need to take the best model from `cvModel` and generate predictions for the entire dataset (dfDelays), then evaluate the best model."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"97bd0fd0-8474-453c-9daa-c9ff6bfa80d6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["bestModel = cvModel.bestModel\nfinalPredictions = bestModel.transform(dfDelays)\nareaUnderRoc = evaluator.evaluate(finalPredictions)\nmlflow.log_metric(\"Final Area Under ROC\", areaUnderRoc)\nareaUnderRoc"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d34afb23-3b11-45d3-baa7-fb35492f166d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finally, we will save this model to disk and end the run."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"310c1316-295d-48c9-801e-6db789056221","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["mlflow.spark.log_model(bestModel, \"model\")\nmodelpath = \"/dbfs/mlflow/mtc/model-dtree-cv\"\nmlflow.spark.save_model(bestModel, modelpath)\nmlflow.end_run()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50029893-b457-438b-a3b0-d4d0749d7f89","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Save the model for batch scoring"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"16810137-21bc-487c-bb79-426fc114a848","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["There are two reasons for saving the model in this lab. The first is so you can access the trained model later if your cluster restarts for any reason, and also from within another notebook. Secondly, we will need to make the model available externally so we can perform batch scoring against it in Exercise 5. Save the model to DBFS so it can be accessed across any clusters in the Databricks Workspace.\n\nNOTE: Save the model in the root of DBFS as this is where Spark Pipelines will look by default."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2be96f47-4bae-41b0-abf6-db1785c97189","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Save the best model under /dbfs/flightDelayModel\nbestModel.write().overwrite().save(\"/flightDelayModel\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2b73510e-df22-45a5-a21b-de95dd33b623","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Next step\n\nContinue to the next notebook, [03 Deploy as Web Service]($./03%20Deploy%20as%20Web%20Service)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ae0978db-5e6b-402d-b34e-6569a0d04cf5","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"04 Train and Evaluate Models","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3103272482370354,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":3103272482370528}},"nbformat":4,"nbformat_minor":0}
